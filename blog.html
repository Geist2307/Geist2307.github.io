<!DOCTYPE html>


<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Blog</title>
    <link rel="stylesheet" href="index.css">

</head>



<body>


    <!-- Navigation bar -->
<nav>
    <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="education.html">Education</a></li>
        <li><a href="portfolio.html">Personal Projects</a></li>
        <li><a href="blog.html">Data Science Corner</a></li>
    </ul>
</nav>


    <section id="blog">
        <h1>Data Science Corner</h1>
        <p>Here are some quick links to some Jupyter Notebooks with key methods in Data Science.</p>

        <!-- Links to tutorial notebooks -->
        <div class="tutorial">
            <h3><a href="https://github.com/Geist2307/DS-Tutorials/blob/master/Notebooks/LaplaceApproximationLogisticRegression.ipynb" target="_blank">
                Tutorial 1: LaPlace Approximation for Logistic Regression</a></h3>
            <p>Basic LaPlace approximation for Logistic Regression</p>
        </div>

        <div class="tutorial">
            <h3><a href="https://github.com/Geist2307/DS-Tutorials/blob/master/Notebooks/FixedBasisRegression_Bayesian.ipynb" target="_blank">
                Tutorial 2: Fixed Basis Regression and Variational Inference</a></h3>
            <p>
               Here I generate some synthethic univariate data and I demonstrate several methods
               from mathematics to code in Julia. Everything implemented from first principles,
               with emphasis on how to translate mathematics to code.
               These are:

               <ul>

                <li>Maximum Likelihood Estimation(MLE)</li>
                <li>Maximum a Posterior (MAP)</li>
                <li> Gibbs Sampling</li>
                <li>Mean Field Variational Inference</li>
               </ul>
            
            </p>
        </div>

        <div class="tutorial">
            <h3><a href="https://github.com/Geist2307/DS-Tutorials/blob/master/Notebooks/BayesLogReg_Metropolis.ipynb" target="_blank">
                Tutorial 3: Bayesian Logistic Regression via Metropolis</a></h3>
            <p>Based on a Kaggle dataset I show:

                 <ul>

                <li>Basic pre-processing to bring data closer to i.i.d form</li>
                <li>From scratch binary Logistic Regression </li>
                <li> Uncertainty Estimation in the Bayesian approach</li>
                <li>Comparison with off-the-shelf package GLM</li>

                </ul>


            </p>
        </div>



       

        <!-- More tutorials to come -->
    </section>

    <footer>
        <p>Â© 2025 Andrei-Ioan Bleahu | All Rights Reserved</p>
    </footer>
</body>
</html>
